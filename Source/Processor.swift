//
//  Processor.swift
//  Preprocessing
//
//  Created by GongXiang on 1/18/18.
//  Copyright Â© 2018 Gix. All rights reserved.
//

import Foundation
import Vision

/// debugger è¾“å‡ºæ¯ä¸€æ­¥çš„å¤„ç†ç»“æœ
public typealias Debugger = (CIImage) -> ()

/// é…ç½®ä¸å¤„ç†å™¨
public struct Configuration {
    
    public static var `default`: Configuration { return Configuration() }
    
    public var colorMonochromeFilterInputColor: CIColor? // CIColorMonochrome kCIInputColorKey å‚æ•°
    public var colorControls: (CGFloat, CGFloat, CGFloat) // CIColorControls Saturation, Brightness, Contrast
    public var exposureAdjustEV: CGFloat // CIExposureAdjust IInputEVKey
    
    public var gaussianBlurSigma: Double
    
    public var smoothThresholdFilter: (CGFloat, CGFloat)? // inputEdgeO, inputEdge1
    
    public var unsharpMask: (CGFloat, CGFloat) // Radius, Intensity
    
    init() {
        colorMonochromeFilterInputColor = CIColor(red: 0.75, green: 0.75, blue: 0.75)
        colorControls = (0.4, 0.2, 1.1)
        exposureAdjustEV = 0.7
        gaussianBlurSigma = 0.4
        smoothThresholdFilter = (0.35, 0.85)
        unsharpMask = (2.5, 0.5)
    }
}

/// é‚£äº›ç±»å‹å¯ä»¥è¯†åˆ«
public protocol Recognizable {
    var croppedMaxRetangle: CorpMaxRetangleResult { get }
}

extension CIImage: Recognizable {
    public var croppedMaxRetangle: CorpMaxRetangleResult {
        return preprocessor.croppedMaxRetangle()
    }
}

extension CGImage: Recognizable {
    public var croppedMaxRetangle: CorpMaxRetangleResult {
        return preprocessor.croppedMaxRetangle()
    }
}

/// æ¯ä¸€æ­¥çš„å¤„ç†ç»“æœ
public protocol Valueable {
    associatedtype T
    var value: T? { get }
}

public struct Value {
    public let image: CIImage
    public let bounds: CGRect
    
    init (_ image: CIImage, _ bounds: CGRect) {
        self.image = image
        self.bounds = bounds
    }
}

public enum Result<T>: Valueable {
    case success(T)
    case failure(PreprocessError)
    
    public var value: T? {
        if case .success(let t) = self {
            return t
        }
        return nil
    }
}

public typealias DivideResult = Result<[Value]>
public typealias CorpMaxRetangleResult = Result<Value>
public typealias FaceCorrectionResult = Result<Value>
public typealias ProcessedResult = Result<Value>

// å¤„ç†å™¨
public protocol Preprocessable { }
public extension Preprocessable {
    var preprocessor: Preprocessor<Self> {
        return Preprocessor(self)
    }
}
public struct Preprocessor<T> {
    let image: T
    init(_ image: T) {
        self.image = image
    }
}

extension CIImage: Preprocessable {}
extension CGImage: Preprocessable {}
extension Value: Preprocessable {
    public var preprocessor: Preprocessor<CIImage> {
        return image.preprocessor
    }
}

public extension Valueable where T == Value {
    public func process(conf: Configuration = Configuration.`default`, debugger: Debugger? = nil) -> ProcessedResult {
        return value?.preprocessor.process(conf: conf, debugger: debugger) ?? .failure(.notFound)
    }
    
    public func divideText(result resize: CGSize? = nil, adjustment: Bool = false, debugger: Debugger? = nil) -> DivideResult {
        return value?.preprocessor.divideText(result: resize, adjustment: adjustment, debugger: debugger) ?? .failure(.notFound)
    }
    
    public func correctionByFace() -> FaceCorrectionResult {
        return value?.preprocessor.correctionByFace() ?? .failure(.notFound)
    }
}

@available(OSX 10.13, iOS 11.0, *)
public extension Preprocessor where T: CGImage {
    
    public func process(conf: Configuration = Configuration.`default`, debugger: Debugger? = nil) -> ProcessedResult {
        return CIImage(cgImage: image).preprocessor.process(conf: conf, debugger: debugger)
    }
    
    public func divideText(result resize: CGSize? = nil, adjustment: Bool = false, debugger: Debugger? = nil) -> DivideResult {
        
        let detectTextRequest = VNDetectTextRectanglesRequest()
        detectTextRequest.reportCharacterBoxes = true
        
        let handler = VNImageRequestHandler(cgImage: image, options: [:])
        try? handler.perform([detectTextRequest])
        
        guard let textObservations = detectTextRequest.results as? [VNTextObservation] else {
            return .failure(.notFound)
        }
        
        let ciImage = CIImage(cgImage: image)
        var results = [Value]()
        
        for textObservation in textObservations {
            guard let cs = textObservation.characterBoxes else { continue }
            
            for c in cs {
                let imageWidth = CGFloat(ciImage.extent.width)
                let imageHeight = CGFloat(ciImage.extent.height)
                // å‘å‘¨å›´å¤šå–2ä¸ªç‚¹
                let x = c.boundingBox.origin.x * imageWidth - 2
                let y = c.boundingBox.origin.y * imageHeight - 2
                let width = c.boundingBox.size.width * imageWidth + 4
                let height = c.boundingBox.size.height * imageHeight + 4
                
                let rect = CGRect(x: x, y: y, width: width, height: height)
                
                var image = ciImage.cropped(to: rect)
                if let size = resize {
                    // å°†æ–‡å­—åˆ‡å‰²å‡ºæ¥ ç¼©æ”¾åˆ°`size`
                    image = image.applyingFilter("CILanczosScaleTransform",
                                                 parameters: [kCIInputScaleKey: size.height / height,
                                                              kCIInputAspectRatioKey: size.width / (width * size.height / height)])
                }
                
                debugger?(image)
//                if adjustment {
//                    image = SmoothThresholdFilter(image, inputEdgeO: 0.15, inputEdge1: 0.9).outputImage ?? image
//                    debugger?(image)
//                    image = AdaptiveThresholdFilter(image).outputImage ?? image
//                    debugger?(image)
//                }
                results.append(Value(image, rect))
            }
        }
        
        return .success(results)
    }
    
    public func croppedMaxRetangle() -> CorpMaxRetangleResult {
        
        let request = VNDetectRectanglesRequest()
        let handler = VNImageRequestHandler(cgImage: image, options: [:])
        
        do {
            try handler.perform([request])
        } catch (let error) {
            return .failure(.inline(error))
        }
        
        guard let observations = request.results as? [VNRectangleObservation] else {
            return .failure(.notFound)
        }
        
        guard let maxObservation = (observations.max(by: { (left, right) -> Bool in
            return left.boundingBox.area > right.boundingBox.area
        })) else {
            return .failure(.notFound)
        }
        
        let ciImage = CIImage(cgImage: image)
        let size = ciImage.extent.size
        let boundingBox = maxObservation.boundingBox.scaled(to: size)
        if ciImage.extent.contains(boundingBox) {
            // Rectify the detected image and reduce it to inverted grayscale for applying model.
            let topLeft = maxObservation.topLeft.scaled(to: size)
            let topRight = maxObservation.topRight.scaled(to: size)
            let bottomLeft = maxObservation.bottomLeft.scaled(to: size)
            let bottomRight = maxObservation.bottomRight.scaled(to: size)
            let outputImage = ciImage.cropped(to: boundingBox)
                .applyingFilter("CIPerspectiveCorrection", parameters: [
                    "inputTopLeft": CIVector(cgPoint: topLeft),
                    "inputTopRight": CIVector(cgPoint: topRight),
                    "inputBottomLeft": CIVector(cgPoint: bottomLeft),
                    "inputBottomRight": CIVector(cgPoint: bottomRight)
                    ])
            return .success(Value(outputImage, boundingBox))
        }
        return .failure(.notFound)
    }
    
    
    public func correctionByFace() -> FaceCorrectionResult {
        return CIImage(cgImage: image).preprocessor.correctionByFace()
    }
}

@available(OSX 10.13, iOS 11.0, *)
public extension Preprocessor where T: CIImage {
    
    /// è¿”å›å¤„ç†åçš„CIImageå¯¹è±¡
    ///
    /// - parameter image: å°†è¦å¤„ç†çš„CIImageå¯¹è±¡ï¼Œæ³¨æ„imageçš„orientation
    /// - parameter configuration: æ ¹æ®è‡ªå·±ç‰¹å®šä¸šåŠ¡ä¸‹çš„å›¾ç‰‡ç‰¹ç‚¹ï¼Œå¯ä»¥è°ƒæ•´ç›¸åº”çš„é¢„å¤„ç†å‚æ•°
    /// - parameter debugger: è¿”å›æ¯ä¸€æ­¥çš„å¤„ç†ç»“æœ
    ///
    /// - returns: retrun processed iamge.
    ///
    public func process(conf: Configuration = Configuration.`default`, debugger: Debugger? = nil) -> ProcessedResult {
        
        var inputImage: CIImage = self.image
        
        // åªæœ‰åœ¨ä¸»åŠ¨è®¾ç½®çš„æ—¶å€™æ‰ä¸¢å¼ƒé¢œè‰²ä¿¡æ¯
        if let color = conf.colorMonochromeFilterInputColor {
            // 0x00. ç°åº¦å›¾ --> ä¸»è¦ç”¨æ¥åšæ–‡å­—è¯†åˆ«æ‰€ä»¥ç›´æ¥å»æ‰è‰²å½©ä¿¡æ¯
            inputImage = inputImage.applyingFilter("CIColorMonochrome", parameters: [kCIInputColorKey: color])
            debugger?(inputImage)
        }
        
        // 0x01. æå‡äº®åº¦ --> ä¼šæŸå¤±ä¸€éƒ¨åˆ†èƒŒæ™¯çº¹ç† é¥±å’Œåº¦ä¸èƒ½å¤ªé«˜
        inputImage = inputImage.applyingFilter("CIColorControls", parameters: [
            kCIInputSaturationKey: conf.colorControls.0,
            kCIInputBrightnessKey: conf.colorControls.1,
            kCIInputContrastKey: conf.colorControls.2])
        debugger?(inputImage)
        
        // 0x02 æ›å…‰è°ƒèŠ‚
        inputImage = inputImage.applyingFilter("CIExposureAdjust", parameters: [kCIInputEVKey: conf.exposureAdjustEV])
        debugger?(inputImage)
        
        // 0x03 é«˜æ–¯æ¨¡ç³Š
        inputImage = inputImage.applyingGaussianBlur(sigma: conf.gaussianBlurSigma)
        debugger?(inputImage)
        
        if let sf = conf.smoothThresholdFilter {
            // 0x04. å»ç‡¥
            inputImage = SmoothThresholdFilter(inputImage,
                                               inputEdgeO: sf.0,
                                               inputEdge1: sf.1).outputImage ?? inputImage
            debugger?(inputImage)
        }
        
        // 0x05 å¢å¼ºæ–‡å­—è½®å»“
        inputImage = inputImage.applyingFilter("CIUnsharpMask",
                                               parameters: [kCIInputRadiusKey: conf.unsharpMask.0, kCIInputIntensityKey: conf.unsharpMask.1])
        debugger?(inputImage)
        
        return .success(Value(inputImage, inputImage.extent))
    }
    
    var cgImage: CGImage? {
        var context: CIContext
        if let device = MTLCreateSystemDefaultDevice() {
            context = CIContext(mtlDevice: device)
        } else {
            context = CIContext()
        }
        return context.createCGImage(image, from: image.extent)
    }
    
    /// å°†ä¸€æ•´ä¸ªæ–‡å­—å›¾ç‰‡åˆ’åˆ†ä¸ºå•ä¸ªçš„`å­—`
    ///
    /// - parameter result: resizeåˆ†å‰²åå•ä¸ªå­—çš„size
    /// - parameter adjustment: æ˜¯å¦å¯¹è°ƒæ•´åˆ†å‰²åçš„å›¾ç‰‡
     /// - parameter debugger: è¿”å›æ¯ä¸€æ­¥çš„å¤„ç†ç»“æœ
    ///
    /// - returns: è¿”å›åˆ†å‰²ç»“æœ
    ///
    public func divideText(result resize: CGSize? = nil, adjustment: Bool = true, debugger: Debugger? = nil) -> DivideResult {
        guard let cgImage = cgImage else {
            return .failure(.abort("size is empty or too big, please double check your image extend. \(image.extent)"))
        }
        return cgImage.preprocessor.divideText(result: resize, adjustment: adjustment, debugger: debugger)
    }
    
    /// å°†å›¾ç‰‡ä¸­æœ€å¤§çš„çŸ©å½¢åˆ‡å‰²å‡ºæ¥
    ///
    ///
    public func croppedMaxRetangle() -> CorpMaxRetangleResult {
        guard let cgImage = cgImage else {
            return .failure(.abort("size is empty or too big, please double check your image extend. \(image.extent)"))
        }
        return cgImage.preprocessor.croppedMaxRetangle()
    }

    /// æ ¹æ®è„¸éƒ¨ä¿¡æ¯çŸ«æ­£å›¾ç‰‡ï¼Œç¡®è®¤è„¸éƒ¨æ­£é¢å‘ä¸ŠğŸ‘†
    ///
    ///
    public func correctionByFace() -> FaceCorrectionResult {
        
        let detector = CIDetector(ofType: CIDetectorTypeFace, context: nil, options: [CIDetectorAccuracy: CIDetectorAccuracyHigh])!
        
        var orientation: CGImagePropertyOrientation = image.extent.width > image.extent.height ? .up : .right
        
        var faceFeatures = detector.features(in: image, options: [CIDetectorImageOrientation: orientation.rawValue])
        
        if orientation == .right {
            let newFeatures = detector.features(in: image, options: [CIDetectorImageOrientation: CGImagePropertyOrientation.left.rawValue])
            if newFeatures.count != 0 {
                if faceFeatures.count == 0 {
                    faceFeatures = newFeatures
                    orientation = .left
                } else {
                    if faceFeatures.first!.bounds.height > newFeatures.first!.bounds.height {
                        faceFeatures = newFeatures
                        orientation = .left
                    }
                }
            }            
        }
        
        guard var faceFeature = faceFeatures.first as? CIFaceFeature,
            faceFeature.hasLeftEyePosition &&
                faceFeature.hasRightEyePosition &&
                faceFeature.hasMouthPosition &&
                !faceFeature.leftEyeClosed &&
                !faceFeature.rightEyeClosed
            else {
                return .failure(.notFound)
        }
        
        if orientation == .up && faceFeature.bounds.height > image.extent.height * 0.4 {
            if let newF = detector.features(in: image, options: [CIDetectorImageOrientation: CGImagePropertyOrientation.down.rawValue]).first as? CIFaceFeature {
                orientation = .down
                faceFeature = newF
            }
        }
        
        let bounds = faceFeature.bounds.applying(image.orientationTransform(for: orientation))
        
        return .success(Value(image.oriented(orientation), bounds))
    }
}
